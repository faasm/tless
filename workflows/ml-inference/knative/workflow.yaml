# -----
# This YAML describes the ML Inference workflow for TLess in Knative.
#
# See here for more details on the workflow:
# https://github.com/faasm/experiment-accless/blob/main/workflows/ml-inference/README.md
# -----
apiVersion: messaging.knative.dev/v1
kind: Channel
metadata:
  name: ingress-to-partition
  namespace: accless
spec:
  channelTemplate:
    apiVersion: messaging.knative.dev/v1
    kind: InMemoryChannel
---
apiVersion: messaging.knative.dev/v1
kind: Channel
metadata:
  name: ingress-to-load
  namespace: accless
spec:
  channelTemplate:
    apiVersion: messaging.knative.dev/v1
    kind: InMemoryChannel
---
apiVersion: messaging.knative.dev/v1
kind: Channel
metadata:
  name: pre-inf-to-predict
  namespace: accless
spec:
  channelTemplate:
    apiVersion: messaging.knative.dev/v1
    kind: InMemoryChannel
---
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: ml-inference-partition
  namespace: accless
spec:
  template:
    spec:
      runtimeClassName: ${RUNTIME_CLASS_NAME}
      containers:
        - image: sc2cr.io/accless-knative-worker:${ACCLESS_VERSION}
          ports:
            - containerPort: 8080
          workingDir: /workflows/ml-inference/knative
          command: [ "./target/release/accless-cloudevent-handler" ]
          env:
            - name: ACCLESS_MODE
              value: "${ACCLESS_MODE}"
    metadata:
      annotations:
        io.containerd.cri.runtime-handler: ${RUNTIME_CLASS_NAME}
      labels:
        accless.workflows/name: ml-inference-partition
---
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: ml-inference-load
  namespace: accless
spec:
  template:
    spec:
      runtimeClassName: ${RUNTIME_CLASS_NAME}
      containers:
        - image: sc2cr.io/accless-knative-worker:${ACCLESS_VERSION}
          ports:
            - containerPort: 8080
          workingDir: /workflows/ml-inference/knative
          command: [ "./target/release/accless-cloudevent-handler" ]
          env:
            - name: ACCLESS_MODE
              value: "${ACCLESS_MODE}"
    metadata:
      annotations:
        io.containerd.cri.runtime-handler: ${RUNTIME_CLASS_NAME}
      labels:
        accless.workflows/name: ml-inference-load
---
apiVersion: sinks.knative.dev/v1alpha1
kind: JobSink
metadata:
  name: ml-inference-predict
  namespace: accless
spec:
  job:
    spec:
      completions: 1
      parallelism: 1
      backoffLimit: 4
      # Clean-up jobs once they are finished
      ttlSecondsAfterFinished: 30
      template:
        spec:
          runtimeClassName: ${RUNTIME_CLASS_NAME}
            # restartPolicy: OnFailure
          restartPolicy: Never
          containers:
            - name: main
              image: sc2cr.io/accless-knative-worker:${ACCLESS_VERSION}
              ports:
                - containerPort: 8080
              workingDir: /workflows/ml-inference/knative
              command: [ "./target/release/accless-cloudevent-handler" ]
              env:
                - name: CE_FROM_FILE
                  value: "on"
                - name: ACCLESS_MODE
                  value: "${ACCLESS_MODE}"
        metadata:
          annotations:
            io.containerd.cri.runtime-handler: ${RUNTIME_CLASS_NAME}
          labels:
            accless.workflows/name: ml-inference-predict
---
apiVersion: messaging.knative.dev/v1
kind: Subscription
metadata:
  name: edge-one-subscription
  namespace: accless
spec:
  channel:
    apiVersion: messaging.knative.dev/v1
    kind: Channel
    name: ingress-to-partition
  reply:
    ref:
      apiVersion: messaging.knative.dev/v1
      kind: InMemoryChannel
      name: pre-inf-to-predict
  subscriber:
    ref:
      apiVersion: serving.knative.dev/v1
      kind: Service
      name: ml-inference-partition
---
apiVersion: messaging.knative.dev/v1
kind: Subscription
metadata:
  name: edge-two-subscription
  namespace: accless
spec:
  channel:
    apiVersion: messaging.knative.dev/v1
    kind: Channel
    name: ingress-to-load
  reply:
    ref:
      apiVersion: messaging.knative.dev/v1
      kind: InMemoryChannel
      name: pre-inf-to-predict
  subscriber:
    ref:
      apiVersion: serving.knative.dev/v1
      kind: Service
      name: ml-inference-load
---
apiVersion: messaging.knative.dev/v1
kind: Subscription
metadata:
  name: edge-three-subscription
  namespace: accless
spec:
  channel:
    apiVersion: messaging.knative.dev/v1
    kind: Channel
    name: pre-inf-to-predict
  subscriber:
    ref:
      apiVersion: sinks.knative.dev/v1alpha1
      kind: JobSink
      name: ml-inference-predict
---
