# -----
# This YAML describes the word-count workflow for TLess in Knative.
#
# This workflow describes a fan-out, fan-in, pattern. The driver service
# fans-out, instantiatating many mapper functions. Then the reducer function
# waits for all mapper functions to finish, and aggreagates the results.
# -----
# We need to have as many channels as edges in our workflow DAG. Alternatively,
# we could enforce edges by using a Broker/Trigger pattern and filtering on
# CloudEvent properties
apiVersion: messaging.knative.dev/v1
kind: Channel
metadata:
  name: ingress-to-splitter
  namespace: accless
spec:
  channelTemplate:
    apiVersion: messaging.knative.dev/v1
    kind: InMemoryChannel
---
apiVersion: messaging.knative.dev/v1
kind: Channel
metadata:
  name: splitter-to-mapper
  namespace: accless
spec:
  channelTemplate:
    apiVersion: messaging.knative.dev/v1
    kind: InMemoryChannel
---
apiVersion: messaging.knative.dev/v1
kind: Channel
metadata:
  name: mapper-to-reducer
  namespace: accless
spec:
  channelTemplate:
    apiVersion: messaging.knative.dev/v1
    kind: InMemoryChannel
---
# We can re-use the same image for all our steps in the chain. Depending on
# the CloudEvent metadata the image will do one thing or another. In
# addition, the channel and subscription structure enforces the right routing
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: word-count-splitter
  namespace: accless
spec:
  template:
    spec:
      runtimeClassName: ${RUNTIME_CLASS_NAME}
      containers:
        - image: sc2cr.io/accless-knative-worker:${ACCLESS_VERSION}
          ports:
            - containerPort: 8080
          workingDir: /workflows/word-count/knative
          command: [ "./target/release/accless-cloudevent-handler" ]
          env:
            - name: TLESS_MODE
              value: "${TLESS_MODE}"
    metadata:
      labels:
        accless.workflows/name: word-count-splitter
      annotations:
        # NOTE: we may have to enable this annotation in Kata's config file
        # under hypervisor.qemu.enable_annotations (add 'default_memory')
        io.katacontainers.config.hypervisor.default_memory: "6144"
---
# JobSink guarantees one Job per CloudEvent, satisfying our dynamic scale-up
# requirements. However, JobSink's propagate CloudEvents through a volume
# mount, rather than an HTTP request.
apiVersion: sinks.knative.dev/v1alpha1
kind: JobSink
metadata:
  name: word-count-mapper
  namespace: accless
spec:
  job:
    spec:
      completions: 1
      parallelism: 1
      backoffLimit: 4
      # Clean-up jobs once they are finished
      ttlSecondsAfterFinished: 30
      template:
        spec:
          runtimeClassName: ${RUNTIME_CLASS_NAME}
          restartPolicy: Never
          containers:
            - name: main
              # WARNING: JobSink seems to, unlike Services, not pick up changes
              # in a tag's digest. So, while we are heavily developing, we
              # should stick to specifying the image via tag digest. Eventually,
              # we can move to a pinned tag
              image: sc2cr.io/accless-knative-worker@sha256:
              ports:
                - containerPort: 8080
              workingDir: /workflows/word-count/knative
              command: [ "./target/release/accless-cloudevent-handler" ]
              env:
                - name: CE_FROM_FILE
                  value: "on"
                - name: TLESS_MODE
                  value: "${TLESS_MODE}"
        metadata:
          labels:
            accless.workflows/name: word-count-mapper
          annotations:
            # NOTE: we may have to enable this annotation in Kata's config file
            # under hypervisor.qemu.enable_annotations (add 'default_memory')
            io.katacontainers.config.hypervisor.default_memory: "6144"
---
# For this last service, we want to give it a high-grace period to make sure
# that the same instance processes all of the events
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: word-count-reducer
  namespace: accless
spec:
  template:
    spec:
      runtimeClassName: ${RUNTIME_CLASS_NAME}
      containers:
        - image: sc2cr.io/accless-knative-worker:${ACCLESS_VERSION}
          ports:
            - containerPort: 8080
          workingDir: /workflows/word-count/knative
          command: [ "./target/release/accless-cloudevent-handler" ]
          env:
            - name: TLESS_MODE
              value: "${TLESS_MODE}"
    metadata:
      labels:
        accless.workflows/name: word-count-reducer
      annotations:
        autoscaling.knative.dev/scale-to-zero-pod-retention-period: "1m"
        # NOTE: we may have to enable this annotation in Kata's config file
        # under hypervisor.qemu.enable_annotations (add 'default_memory')
        io.katacontainers.config.hypervisor.default_memory: "6144"
---
apiVersion: messaging.knative.dev/v1
kind: Subscription
metadata:
  name: edge-one-subscription
  namespace: accless
spec:
  channel:
    apiVersion: messaging.knative.dev/v1
    kind: Channel
    name: ingress-to-splitter
  reply:
    ref:
      apiVersion: messaging.knative.dev/v1
      kind: InMemoryChannel
      name: splitter-to-mapper
  subscriber:
    ref:
      apiVersion: serving.knative.dev/v1
      kind: Service
      name: word-count-splitter
---
apiVersion: messaging.knative.dev/v1
kind: Subscription
metadata:
  name: edge-two-subscription
  namespace: accless
spec:
  channel:
    apiVersion: messaging.knative.dev/v1
    kind: Channel
    name: splitter-to-mapper
  subscriber:
    ref:
      apiVersion: sinks.knative.dev/v1alpha1
      kind: JobSink
      name: word-count-mapper
---
apiVersion: messaging.knative.dev/v1
kind: Subscription
metadata:
  name: edge-three-subscription
  namespace: accless
spec:
  channel:
    apiVersion: messaging.knative.dev/v1
    kind: Channel
    name: mapper-to-reducer
  subscriber:
    ref:
      apiVersion: serving.knative.dev/v1
      kind: Service
      name: word-count-reducer
---
